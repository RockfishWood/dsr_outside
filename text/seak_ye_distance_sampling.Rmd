---
title: "SEAK YE Distance Sampling in R"
author: "Andrew Olson"
date: "December 11, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r, echo = FALSE}
library(png)
library(knitr)
```

## Distance Sampling

The `Distance` package is used to estimate population densities and abundances using line and point transect sampling methodology. For Southeast Alaska yelloweye rockfish (YE) we conduct line transects using an remotely operated vehicle (ROV) to conduct line transects in YE rocky habitat and conduct video review to determine species ID and lengths.

Distance sampling has 3 key assumptions that include:

**1. Objects on the line or point are detected with certainty;**
  
  i) We detect fish on both sides of the transect line using the ROV stereo left and right cameras and the belly camera is used to detect any fish that may have been overlooked that we cold not initially detect with the stereo cameras.  

**2. Objects do not move;**

  i) As the ROV is moving we will often see fish in a resting state or moving as the ROV approaches and are assumming the ROV has no behavioral affect on fish, therefore it is important to ensure fish are detected on their first appearance.  

**3. Measurements are exact.**

  i) During video review you collect measurements from the transect line that are converted within the Eventmeasure software from a radial measurement (distance measurement from ROV point of view) to a perpendicular measurement (distance measurement from ROV side view).
  
## ROV Survey Data

Survey data consists of multiple tables that will need to be summarized and joined.  You will be working with 3 main tables in order to produce a valid density estimate which consist of:

**1. ROV Navigation Table**
    
  i) This table consists of the dive number, time (UTC seconds), vessel and ROV position in UTM projection (meters), ROV heading, etc.  We are mainly concerned with the dive number, time, and ROV position as this will tells us when and where the ROV is while we are conducting video review.  
    
**2. Specimen Table**

  i) This is created during the video review process in the EventMeasure software and consists of video file name, Time (use UTC seconds), length (mm) measurement (YE only), precision and direction of length measurement, x, y, z distance from the transect line (MID_X represents the perpendicular distance from the transect line at which a fish was detected), species ID (DSR, lingcod, and black rockfish), specimen number, stage (adult, subadult or juvenile used for YE only), and activity.

**3. Video Quality Control**

  i) Reviews quality of video for good and bad segments for each line transect that is then used in determining good length transects from the ROV nav data and consists of video filename, video frame, time (UTC seconds), dive number, video quality (good or bad), video quality code, description of good and bad assignments (i.e. good going forward, ROV loitering in same area, going over a large drop-off, etc.), and a start and end assignment for each transect. 
  
```{r, echo=FALSE}
library(DiagrammeR)
mermaid("
graph TD
    A[ROV Survey] --> B[Navigation Data]
    A[ROV Survey] --> C[Video Review]
    C[Video Review] --> D[EventMeasure]
    D[EventMeasure Software] --> E[Species ID]
    D[EventMeasure Software] --> F[Species Length]
    D[EventMeasure Software] --> G[Quality Control]
    B[Navigation Data] --> H[Transect Length Estimation]
    G[Quality Control] --> H[Transect Length Estimation]
    E[Species ID] --> I[Specimen Data]
    F[Species Length] --> I[Specimen Data]
    H[Transect Length Estimation] --> J[Survey Data]
    I[Specimen Data] --> J[Survey Data] 
    

")
```

## Getting Started...

The remaining instructions assume that all necessary tables (navigation, quality control, and specimen) are ready for analysis.  For file names make sure you use a standard naming convention that includes year, survey area, and data type (ex. QC_SSEO_2018, NAV_SSEO_2018, etc.) and for tables ensure columns have the same naming convention i.e. DIVE_NO, DIVE, etc. just pick one otherwise trying to join tables will require more work. 

### Project Setup in R

You will need to create a new project in R with folders pertaining to your data (fishery and survey), R scripts, etc. to create a workflow that is easy to follow and reproducible for future use.  With the ROV surveys you will want to create folders for each survey area and year so data is organized for a given survey year (ex. dsr_survey --> data --> survey --> sseo --> 2018)

INCLUDE PICTURES OF SETTING UP AN R PROJECT HERE AND IMAGES OF FOLDER SETUP

After you have setup your R project you will need add a new R script file labeled **dsr_density.R** and will be used to produce a density estimate for an ROV survey. For the density analysis you will need install and reference the following packages: `tidyverse`, `Distance`, `lubridate`, and `zoo` using `install.packages("")` and `library()`:

```{r, message=FALSE}
#Reference installed packages
library(tidyverse)
library(Distance) 
library(lubridate) 
library(zoo)
```

To make notes throughout your R script on what you are doing and why, use the `#` symbol and R recognizes this as text and will not affect your actual code. 

## Yelloweye Rockfish Density Estimate
For the following density estimate we will use the 2018 SSEO ROV survey data as our example.  After you have loaded the necessary packages to begin your analysis you need to import the survey data which includes: navigation, quality control, and specimen tables.  Each table will need to have a naming assignment using the `->` or `<-` symbol in order for use to reference the data anytime without re-importing the data each time. 

```{r, message = FALSE, warning = FALSE}
#Import navigation, quality control, and specimen data
nav_sseo <- read_csv("data/survey/sseo/2018/2018_sseo_nav_data.csv")
qc_sseo <- read_csv("data/survey/sseo/2018/2018_sseo_qc.csv")
species_sseo <- read_csv("data/survey/sseo/2018/2018_sseo_species.csv")
```
Verify that the data imported correctly using `View()` this will open the data tables in a spreadsheet tab.  You can also view the data within the R console by typing in the data table name, see a summary of the data using `summary()` and look at column attributes using `glimpse()`.


### Transect Line Estimation
Now that we know the data imported correctly we will start with cleaning up the navigation data by joining the **Navigation Table** with the **Quality Control Table**.  In order to do this we need edit the column headers in order to join the tables together by Dive number and time (seconds).  Make sure you verify the table join worked correctly by lining up Dive number and seconds between tables.  

```{r, message = FALSE, warning = FALSE}
#Need to get both tables in similar format before joining
#This selects for just the dive numbers in the nav table
nav_sseo %>% mutate(Dive = substr(DIVE_NO, 6, 7)) -> nav_sseo

#Converts the Dive column to be numeric
nav_sseo$Dive <- as.numeric(nav_sseo$Dive)

#Rename column names
plyr::rename(nav_sseo, replace = c("SECONDS" = "Seconds"))-> nav_sseo

#Join Tables using a full_join
transect <- full_join(nav_sseo, qc_sseo, by = c("Dive", "Seconds"))
```

Now that the tables have been joined we need to assign "good" and "bad" sections of the transect for every second the ROV was moving based on the quality control table.  So for example if Dive 1 was considered a good transect from start to end we need to fill in the `NA`s or blanks for the entire time and label them as good. For this part we use functions from the `dplyr` package within `tidyverse` which allows us clean, summarise and manipulate our data.  One of the key functions with `dplyr` is the pipe operator `%>%` which allows us to chain a bunch of commands.    

```{r, message = FALSE, warning = FALSE}
#Need to fill in missing values in nav table from the quality control so the good and bad sections have time assignments for the entire transect
#fill() function automatically replaces NA values with previous value
#use select() to only keep the necessary columns i.e. Seconds, Dive #, x, y, video quality (good/bad)
transect_qc <- transect %>% fill(Family) %>% filter(!is.na(Family)) %>% 
  select(Seconds, Dive, ROV_X, ROV_Y, Family)

#Check data
transect_qc

#Output your cleaned up data table
write.csv(transect_qc, file = "output/transect_qc.csv")
```

Our navigation table should be cleaned up so each dive has an associated time (seconds), ROV position (x, y) and a good or bad assignment.  So let's see what the data looks like using ``ggplot()`` and see where the good and bad sections for each dive are at and ROV path.


```{r, echo = FALSE, message = FALSE, warning = FALSE}
transect_qc2 <- transect_qc %>% filter(Dive == 1:4)
```

In this example, I have reduced the number of Dives to graph so they are able to fit on the page.  For the rest of the analysis you will still utilize the `transect_qc` table.  Looking at the graph we see that dives 1-3 are fairly straight while dive 4 has a zig-zag pattern.  There are a couple of possiblities why dive 4 is displaying this pattern which include:

1. Navigation data is bad and most likely resulted from tracking computer issues in the field;
2. Graph is not scaled correctly in R.  Graphs are often compressed in R and may not display correctly.  Import data into ArcGIS using a UTM projection to determine if the ROV route is straight or not.  

If the dive does not follow a straight pattern in ArcGIS then it is a bad dive and must be excluded from the analysis. Verify for all remaining dives in the data in the `transect_qc` table.  

```{r, message = FALSE, warning = FALSE}
ggplot(transect_qc2, aes(ROV_X, ROV_Y)) + 
  geom_point(aes(colour = factor(Family))) +
  facet_wrap(~Dive, scales = "free") +
  theme(axis.text.x = element_text(angle = 90))
```

#### Smoothing Line Transect Data
Now that all the line transects have been verified in ArcGIS you should have noticed that each measurement is a point in time and space and now we need to "connect-the-dots" by fitting a line to the data using a smoothing function to each dive.  So first we must convert our `Dive` column from a numric variable to a factor. 

```{r, message = FALSE, warning = FALSE}
transect_qc$Dive <- factor(transect_qc$Dive)

#Verify Dive column was converted correctly
glimpse(transect_qc)
```

Next we will create a function that fits a smooth line and extracts the predicted values from the smooth fit for each dive.  We will have the output save as a .pdf file for easy viewing since we will have multiple dives to review.

```{r, message = FALSE, warning = FALSE}
pdf("output/2018_sseo_smoothed_transects.pdf")

#Sets up your graph window as a 2 x 2 frame
par(mfrow = c(2, 2))

#Smoothing function
for (i in 1:length (levels (transect_qc$Dive))) {
  sT <- transect_qc [transect_qc$Dive == levels (transect_qc$Dive)[i],]
  
  tX <- smooth.spline(sT$Seconds, sT$ROV_X, spar = 0.7)
  tY <- smooth.spline(sT$Seconds, sT$ROV_Y, spar = 0.7)
  pX <- predict (tX) ##gives the predicted values for lat by seconds for each observation at the level 
  pY <- predict (tY) ##gives the predicted values for long by seconds for each observation at the level 
  
  prSp <- data.frame (pX, Y = pY$y) ## creates the data frame with the pX values of seconds=x and y=lat and the pY values of long=Y
  names (prSp) <- c("Sec", "X", "Y")##renames seconds=Sec, y=X, and Y=Y
  
  plot (sT$ROV_X, sT$ROV_Y, main = levels (transect_qc$Dive)[i],asp = 1, ylab = "Y", xlab = "X")#plots the observed unsmoothed points for lat (x) vs.long (y)
  lines (prSp$X, prSp$Y, lwd = 2, col = 5)  #draws the predicted line for lat vs. long (2=red and 5=blue) 
  
  ## output 
  if (i == 1){
    outPut <- prSp
  } else {
    outPut <- rbind (outPut, prSp) 
  }
}

dev.off()

#Combines your original data with the predicted output from smoothing function
transect_pred <- cbind(transect_qc, predX = outPut$X, predY = outPut$Y)

#Check and export data to be used in ArcGIS
View(transect_pred)

write.csv(transect_pred, file = "output/2018_sseo_smooth_predict.csv")
```

#### Estimate Transect Lengths in ArcGIS

Here we will take the predicted values for each smoothed transect to estimate length between each predicted value (i.e. distance from 1 to 2, 2 to 3, etc.) using ArcGIS and the extension package ET GeoWizards using the following steps:

**Import the smoothed data into ArcGIS**

  * Convert the 2018_sseo_smooth_predict.csv file to a feature class in ArcGIS
    + Create feature class from XYTable using **pred X** and **pred Y** and project the feature class as wgs84 utm8N or 7N for EYKT and 8N for other areas
  * Check to make sure smoothed data was appeneded to correct transect in R by selecting each transect separately in GIS and make sure each is a distinct segment.
  * Convert points to polylines
    +ET GeoWizards: Convert --> Point to Polyline
    +ID Field: Dive
    +Select Point Order Field: Seconds
    +Select Link Field: Seconds

```{r, echo = FALSE}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/points_to_polylines.png"))

```

*Troubleshoot*: If this does not work, then there could be a problem with the predict x and y, predicting to the correct dives in R.  CHeck to make sure it worked correctly.  If there is a problem with the polylines connecting, then the predict x and y may have been pasted to the wrong place in the data frame in R.  THe file must be sorted by dive number and seconds before running in R.

**Create routes from polylines**

  * Use ArcCatalog "Linear Referencing Tools" - "Create Routes"
  * Route Identifier Field: Dive
  * Make sure that DIVE is formatted as a NUMBER

```{r, echo = FALSE}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/create_routes.png"))
```

**Calibrate routes with points**

  * Use ArcCatalog Linear Referencing Tools --> Calibrate Routes
  * Route Identifier Field: Dive
  * Measure Field: Seconds
  * Search Radius: 1000 meters
  * Check: "Interpolation between calibration points" and "Ignore spatial gaps"


```{r, echo = FALSE}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/calibrate_routes.png"))
```

**Import video quality review data using linear referencing**

  * First, pull up the quality review spreadsheet
  * Create .csv file with "Dive", "From" and "To"

```{r, echo = FALSE}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/linear referencing table.png"))
```

  * The seconds (Sec) will be the “From time” field for each video quality segment but will need to create a “To field” by using the next time entry for each transect. 
  * Save Quality File as .csv or.txt. Doesn’t seem to import correctly as an excel file. 
  * Format dive the same for the calibrated routes and in the QC.
	  + Seems to work best formatted as a number. 
  * In ArcGIS can create a new column for calibrated routes with dive formatted as a number (multiply dive_no by 1). 
    + File --> Add Data --> Add Route Events (Can only do this in ArcGIS - not ArcGIS Pro)
    + _Route Reference_: Calibrated Transects
    + _Route Identifier_: Dive
    + _Event Table_: Video Review To/From file
    + _Route Identifier_: DIve
    + _Choose the type of events the table contains_: Line Events
    + _From-Measure_: From
    + _To-Measure_: To
    
```{r, echo = FALSE}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/add_routes.png"))
```

  * Also check "Generate a field for locating errors" under advanced options.

```{r, echo = FALSE}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/advanced_routes.png"))
```

  * If errors occur check to make sure that Dive is formatted the same for both the calibrated routes and the video quality review data

**Export the GGF only fields form the routed video review data**

**Estimate the length of each segment**

  * Add column to attribute table for lengths
  * Then right click column and calculate geometry
  * Export text file
  * Can then import file into R to sum all the line sgements for each transect to get the total estimated transect lengths
  * Compare the GGF line lengths with the total transect line lengths calculated in GIS and in R
  
**Edit line lengths**

  * Need to query and see which yelloweye fall outsdie the GGF segments
  * If need to adjust quality codes and change GGF segments, then will need to re-route quality data and re-estimate line lengths


**Import estimated length into R for filtering**

Now that you have your route lengths from GIS you need to re-import the data into R and summarise the transect lengths by dive number and exclude out all `NA` values.

```{r, message = FALSE, warning = FALSE}
sseo_transects <- read_csv("data/survey/sseo/2018/2018_sseo_smoothed_transect_lengths.csv")

transect_summary <- sseo_transects %>% group_by(Dive) %>% 
  summarise(total_length_m = sum(Shape_Length, na.rm = TRUE))

```

Check the data for any anomolies.  For 2018 SSEO you will notice that Dive 8 is only ~94 m and our transect lengths are typically 1,000 m, therefore this was a bad dive and the data should be excluded from the analysis. 

```{r, message = FALSE, warning = FALSE}
#Verify dive transect lengths
#Dive number 8 was excluded from the analysis due to loss of navigation for majority of dive
transect_summary <- transect_summary %>% filter(Dive != "8")
```

Congrats, you have estimated line transect lengths that will now be linked with our specimen table in order to produce a density estiamte using a distance analysis!


### Distance Analysis

To begin we need to import our specimen data and `filter()` for YE species and for subadult and adult stages only.  YE ROV assessments from 2012-2013 and from 2014-2017 used an arbitrary length cut-off of 340 and 350 mm to include in the distance analysis based on port sampling data (2012-2014) for a small sample size and incorporated subadult YE if the average length was > 340.  Prior to 2012, assessments were conducted using all subadults and adults that were observed and since the fishery selects for fish much smaller than 340 mm will be using all subadults and adults into assessment for all future distance analyses.  

```{r, message = FALSE, warning = FALSE}
#Import ROV specimen data and filter for YE only

sseo_bio <- read_csv("data/survey/sseo/2018/2018_sseo_species.csv") %>% filter(SPECIES == 145)

#For the density estimate we only want adults and subadults as these are selected for in the fishery
#filter bio data so raw data is only adults and subadults for YE
ye_adult <- sseo_bio %>% filter(STAGE != "JV")
```

Now we need to join our filtered specimen table with our estimated transect length summary.  We need to clean up the column headers in order to join the tables.  Here we will also create a couple of new columns using the `mutate()` function that includes: management area label, area of the survey (km^2), and the perpendicular distance (m) a fish was observed from a transect line.  You will notice in the `MID_X` column we have postive and negative distances and this is due to us observing fish on the left (negative) and right (positive) side of the transect line so we take the absolute value to make all our distances positive.

Note that `NAs` in the distance column is ok this is how we account for zero fish observed during a transect.  If you were to change `NAs` values in the distance column to zero you would be stating that fish were observed on the transect the line which would bias your density estimate high, so in this case `Nas` are a good thing.

```{r, message = FALSE, warning = FALSE}
#Join specimen and transect summary table together
#Columns are renamed to avoid confusion with specimen table
plyr::rename(transect_summary, replace = c("Dive" = "DIVE_NO", "total_length_m" = "transect_length_m")) -> transect_summary

sseo_survey <- full_join(transect_summary, ye_adult, by = "DIVE_NO") %>% 
  mutate(mgt_area = "SSEO", Area = 1056, distance = abs(`MID_X (mm)` * 0.001))


#If you have transects with zero fish observed you need to replace "NAs" with zero for a given transect
sseo_distance <- sseo_survey %>% select(YEAR, mgt_area, Area, DIVE_NO, transect_length_m, distance) %>%
  mutate(YEAR = replace_na(YEAR, 2018)) 
```

Now we will be using the `Distance` package which has a few header naming restrictions in order for the analysis to work so once again we need to rename some column headings.  

```{r, message = FALSE, warning = FALSE}
#These column heading name changes are specific to the Distance package only
plyr::rename(sseo_distance, replace = c("mgt_area" = "Region.Label", "DIVE_NO" = "Sample.Label",
                                     "transect_length_m" = "Effort" )) -> sseo_distance

#Data has to be in a data frame in order to work in distance
as.data.frame(sseo_distance) -> sseo_distance
```

Let's see a summary of our data and and fit a histogram to the distance from the transect line see how frequently we were able to detect fish from the transect line.

```{r, message = FALSE, warning = FALSE}
#View Summary of Data
summary(sseo_distance$distance)

#View Historgram of perpendicular distance from transect line
hist(sseo_distance$distance, xlab = "Distance (m)")

```

Looking at our histogram we were able to detect the most fish closest to the transect line and our detection ability went down the further the fish were from the transect line.  Looking at the histogram our data looks pretty normal and is ready for model fitting.  Sometimes our distance histograms can have odd patterns and are not realistic.  This can often be fixed by establishing bins and cutoff points in order to make the data work nicely.  In regards to the ROV, you may see this as a result of bad lighting, camera issues, etc. and a lot of those issues are resolved in the data prep we did earlier. Here is an example of non-ideal data:  

```{r, echo = FALSE, out.width = '75%'}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/bad_data.png"))
```

Now we want to fit a probabality detection function which describes the relationship between distance and the probablity of detecting a fish. When selecting a model we have a few items to consider which include:

1. **Model Robustness**-use a model that will fit a wide variety of plausible shapes for g(x);
2. **Shape Criterion**-use a model a shoulder;
3. **Pooling Robustness**-use a model for the average deterction function, even when many factors affect detectability;
4. **Estimator Efficiency**-use a model that will lead to a precise estimator of density.

```{r, echo = FALSE, out.width = '75%'}
knitr::include_graphics(rep("H:/Groundfish/ROCKFISH/DSR/dsr_safe/figures/instructions/prob_detection.png"))
```

We will be fitting different models to explore and compare which model provides the best fit to data, which we test using a goodness-of-fit test, QQ-plot and the Akakie Information Criterion (AIC)

Our model fits are broken down into **key functions** and **adjustment terms**:

**Key Functions**
  * Uniform
  * Half-Normal
  * Hazard Rate
  
**Adjustment Term**
  * Cosine
  * Hermite Polynomial
  * Simple Polynomial
  
Note that the more parameters you fit the great the flexibility there is fitting the function, however uncertainty increases.  

Now lets fit different models with key functions and adjustment terms.  `convert.units` is used to convert our survey area km^2 into meters so all our measurements are in the same unit 

```{r, message = FALSE, warning = FALSE}

#Distance Model fitting
sseo.model1 <- ds(sseo_distance, key = "hn", adjustment = NULL,
                  convert.units = 0.000001)

summary(sseo.model1$ddf)

plot(sseo.model1)

#Cosine Adjustment
sseo.model2 <- ds(sseo_distance, key = "hn", adjustment = "cos",
                  convert.units = 0.000001)

summary(sseo.model2)

plot(sseo.model2)

#Hazard key function with Hermite polynomial adjustment
sseo.model3 <- ds(sseo_distance, key = "hr", adjustment = "herm",
                  convert.units = 0.000001)

summary(sseo.model3)

plot(sseo.model3)
```

Comparing the model fit plots and AIC values we see that Model #1 (half-normal without an adjustment term) fits ok and does not have the best shoulder.  Looking at Models #2 (half-normal with cosine adjustment) and #3(Hazard rate with hermite polynomial) we see that these models have more flexiblity in their fits which creates a much nicer shoulder.  Both models #2 and #3 are very similar and when we compare the AIC values model #2 has the best fit (AIC= 544.3522) compared to model #3 (AIC= 544.5506).  

Since models #2 and #3 are very similar lets check their QQ-plot and test the goodness of fit.  

```{r, message = FALSE, warning = FALSE}
gof_ds(sseo.model2)

gof_ds(sseo.model3)

```
Again you will see that the results are very similar between models #2 and #3, but since model #2 had the best AIC value and goodness of fit test statistic and p-value we select model #2 for our density estimate. 

In the model `summary()` you may have noticed that the abundnace and density estimates are part of this output.  We can get the specific density estiamtes using `#Density Estimate
sseo.model2$dht$individuals$D`.  This provides the density estimate for number of fish, standard error, and CV.

```{r, message = FALSE, warning = FALSE, echo = FALSE}
knitr::kable(sseo.model2$dht$individuals$D)

```

Additionally, we need the model summary for the survey area (km^2), meters surveyed (Effort), n (number of fish observed), k (number of transects) and the encounter rate (ye/m) using `sseo.model2$dht$individuals$summary`

```{r, message = FALSE, warning =FALSE, echo = FALSE}
knitr::kable(sseo.model2$dht$individuals$summary)

```

That's it!  

## Future Work and Notes

1. Conduct GIS portion of analysis in R;
2. Use consistent column headers among different tables at the onset of recording data eliminate need to rename column headers in R;
3. Most likely each assessment will need to have it's own seperate R script file that is organized well for tracking and repeatability.
4. Need to work with region 1 programmers on standardizing raw data tables to be put into OceanAK. 
